{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "347af9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ec78e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da371ac3",
   "metadata": {},
   "source": [
    "## LM Studio connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f64f9eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Connected to LM Studio\n",
      "✓ Available model: qwen/qwen3-4b-thinking-2507\n"
     ]
    }
   ],
   "source": [
    "WINDOWS_IP = \"192.168.1.33\"\n",
    "PORT = 1234\n",
    "\n",
    "try:\n",
    "    client = OpenAI(\n",
    "        api_key=\"not-needed\",\n",
    "        base_url=f\"http://{WINDOWS_IP}:{PORT}/v1\"\n",
    "    )\n",
    "    models = client.models.list()\n",
    "    print(\"✓ Connected to LM Studio\")\n",
    "    print(f\"✓ Available model: {models.data[0].id if models.data else 'None'}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Connection error: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c2a351",
   "metadata": {},
   "source": [
    "## Normalize LLM responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf99a5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_llm_response(text):\n",
    "    \"\"\"\n",
    "    Normalize LLM output to 'Attack' or 'Support'\n",
    "    Includes common synonyms and variations\n",
    "    \"\"\"\n",
    "    text = text.strip().upper()\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # Keywords mapping\n",
    "    attack_keywords = [\"ATTACK\", \"OPPOSE\", \"AGAINST\", \"DISAGREE\"]\n",
    "    support_keywords = [\"SUPPORT\", \"AGREE\", \"FOR\", \"PRO\"]\n",
    "\n",
    "    for word in attack_keywords:\n",
    "        if word in text:\n",
    "            return \"Attack\"\n",
    "    for word in support_keywords:\n",
    "        if word in text:\n",
    "            return \"Support\"\n",
    "    \n",
    "    return \"Unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f829f64",
   "metadata": {},
   "source": [
    "## Primer: 4 RBAM predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890a63a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRIMER = [\n",
    "    (\"We should ban plastic bags\", \"Plastic bags harm the environment\", \"Support\"),\n",
    "    (\"We should ban plastic bags\", \"Plastic bags are convenient for shopping\", \"Attack\"),\n",
    "    (\"Smoking is harmful\", \"Governments should ban cigarettes\", \"Support\"),\n",
    "    (\"Immigration is beneficial\", \"Border restrictions harm the economy\", \"Support\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0cd8ff",
   "metadata": {},
   "source": [
    "## Classify function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "471558a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_relation(arg1, arg2):\n",
    "    \"\"\"\n",
    "    Applies the 4-step RBAM-style approach:\n",
    "    1. Add primer examples.\n",
    "    2. Add test pair.\n",
    "    3. Feed prompt to LLM.\n",
    "    4. Normalize output.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1 & 2: Build prompt\n",
    "    prompt = \"Classify the relationship as 'ATTACK' or 'SUPPORT':\\n\\n\"\n",
    "    for ex1, ex2, label in PRIMER:\n",
    "        prompt += f\"Arg1: {ex1}\\nArg2: {ex2}\\nClassification: {label.upper()}\\n\\n\"\n",
    "    \n",
    "    # Step 3: Add test pair\n",
    "    prompt += f\"Arg1: {arg1}\\nArg2: {arg2}\\nClassification:\"\n",
    "    \n",
    "    # Step 4: Call the model\n",
    "    try:\n",
    "        response = client.completions.create(\n",
    "            model=\"qwen/qwen3-4b-thinking-2507\",\n",
    "            prompt=prompt,\n",
    "            max_tokens=8,\n",
    "            temperature=0.0,\n",
    "            stop=[\"Arg1\", \"\\n\\n\"]  # stop if new block starts\n",
    "        )\n",
    "        \n",
    "        raw_text = response.choices[0].text\n",
    "        return normalize_llm_response(raw_text)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Inference error: {e}\")\n",
    "        return \"Unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28552f8b",
   "metadata": {},
   "source": [
    "## Test on small examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "972dad4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TESTING SAMPLE PAIRS\n",
      "============================================================\n",
      "Result: Support | Arg1: Smoking causes health issues. | Arg2: Tobacco should be banned.\n",
      "Result: Support | Arg1: Vaccines are safe. | Arg2: We should mandate vaccinations.\n",
      "Result: Support | Arg1: Immigration is beneficial. | Arg2: Border restrictions harm the economy.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING SAMPLE PAIRS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_pairs = [\n",
    "    (\"Smoking causes health issues.\", \"Tobacco should be banned.\"),\n",
    "    (\"Vaccines are safe.\", \"We should mandate vaccinations.\"),\n",
    "    (\"Immigration is beneficial.\", \"Border restrictions harm the economy.\"),\n",
    "]\n",
    "\n",
    "for arg1, arg2 in test_pairs:\n",
    "    result = classify_relation(arg1, arg2)\n",
    "    print(f\"Result: {result} | Arg1: {arg1} | Arg2: {arg2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c74b90",
   "metadata": {},
   "source": [
    "## Load dataset and shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb07022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset shape: (500, 3)\n",
      "Class distribution:\n",
      "relation\n",
      "Support    253\n",
      "Attack     247\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../../data/kialo/kialo-pairs-50k.csv')\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)  # shuffle\n",
    "df = df.head(500)  # small batch for demo\n",
    "print(f'\\nDataset shape: {df.shape}')\n",
    "print(f'Class distribution:\\n{df[\"relation\"].value_counts()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b5ac4d",
   "metadata": {},
   "source": [
    "## Classify all pairs with progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c57f5dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying: 100%|██████████| 500/500 [00:27<00:00, 17.93it/s]\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "times = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Classifying\"):\n",
    "    start = time.time()\n",
    "    pred = classify_relation(row[\"parent_clean\"], row[\"child_clean\"])\n",
    "    preds.append(pred)\n",
    "    times.append(time.time() - start)\n",
    "\n",
    "df[\"pred\"] = preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51b36f5",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2cefa4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Distribution of predictions ===\n",
      "pred\n",
      "Support    292\n",
      "Attack     208\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Classification Report (Unknown excluded) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Attack       0.71      0.60      0.65       247\n",
      "     Support       0.66      0.76      0.70       253\n",
      "\n",
      "    accuracy                           0.68       500\n",
      "   macro avg       0.68      0.68      0.68       500\n",
      "weighted avg       0.68      0.68      0.68       500\n",
      "\n",
      "\n",
      "Average inference time: 0.06 sec per pair\n",
      "Total time for 500 examples: 0.5 min\n"
     ]
    }
   ],
   "source": [
    "# Print distribution of predictions\n",
    "print(\"\\n=== Distribution of predictions ===\")\n",
    "print(df[\"pred\"].value_counts())\n",
    "\n",
    "# Exclude Unknown for evaluation\n",
    "df_eval = df[df[\"pred\"] != \"Unknown\"]\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"\\n=== Classification Report (Unknown excluded) ===\")\n",
    "print(classification_report(df_eval[\"relation\"], df_eval[\"pred\"]))\n",
    "\n",
    "# Average inference time\n",
    "avg_time = sum(times) / len(times)\n",
    "print(f\"\\nAverage inference time: {avg_time:.2f} sec per pair\")\n",
    "print(f\"Total time for {len(df)} examples: {sum(times)/60:.1f} min\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddc_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
